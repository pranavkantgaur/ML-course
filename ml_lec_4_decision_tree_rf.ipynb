{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml-lec-4-decision-tree-rf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMU7DkAwQAFS7T2xKLmEbR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkantgaur/ML-course/blob/main/ml_lec_4_decision_tree_rf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives\n",
        "â€¢\tEnsemble Learning: Bagging, Boosting, Stacking; Random Forest, AdaBoost, Gradient Boost."
      ],
      "metadata": {
        "id": "fJkcN9quXcVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ensembling technique\n",
        "* Bagging(Bootstrap aggregation)\n",
        "  * Splitting dataset of size n with replacement into m subsets of size k where k <= n, where m is the number of homogenous base-learners\n",
        "  * How to  aggregate the results? Voting, or averaging, ... think.\n",
        "  * Merits:\n",
        "    * Reduces variance, how? \n",
        "  * Locate a dataset which highlights difference between RF and Decision tree and decision tree-GBDT(TODO)  \n",
        "  * Logic behind finding bias (TODO)\n",
        "* Boosting: https://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
        "  * Sequential model building based on data-reweighing (difficult data has higher weight than sinpler data)\n",
        "  * ToDO: \n",
        "     * https://en.wikipedia.org/wiki/Empirical_risk_minimization, one-dimensional optimization\n",
        "     * https://analyticsindiamag.com/adaboost-vs-gradient-boosting-a-comparison-of-leading-boosting-algorithms/\n",
        "  * Merits:\n",
        "    * Reduces bias\n",
        "* Stacking:\n",
        "  * bagging with heterogenous base-learns + meta-classifier\n",
        "  * Merits:\n",
        "    * ??\n",
        "* Cascading: \n",
        "  * Heterogenous boosting"
      ],
      "metadata": {
        "id": "ouW81Kkhp2yQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "\n",
        "# use for both decision and random forest\n",
        "\n"
      ],
      "metadata": {
        "id": "TrQnGd6ypv6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt_MLHygKeZG"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "https://scikit-learn.org/stable/modules/tree.html#tree\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "'''\n",
        "1. What is Decision tree\n",
        "   * Like many nested if-else conditions\n",
        "   * Axis parallel separation boundaries/hyperpl;anes\n",
        "   * Concepts:\n",
        "     * Entropy with example: \n",
        "       * Uniform distb.(higher entropy) vs Normal distb. \n",
        "       * Jar example\n",
        "     * Information gain across levels of decisions tree:\n",
        "     * Gini index \n",
        "   * Merits:\n",
        "     * Highly interprable\n",
        "   * Demerits:\n",
        "     * Text classification, poor perf. why? \n",
        "2. Type of problems addressed? Classification and Regression\n",
        "3. Worked example:\n",
        "   * Classification(outlook/rainy example) with regression(hours played) \n",
        "     * Supppose feature values are non-categorical, how to proceed?\n",
        "4. Where to stop:\n",
        "   1. Pruning approaches:\n",
        "      1. Min. Number of data-points for a tree-node\n",
        "      2. Info. gain/entropy loss threshold     "
      ]
    }
  ]
}